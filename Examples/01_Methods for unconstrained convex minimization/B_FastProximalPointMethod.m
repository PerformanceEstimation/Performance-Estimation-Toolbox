clear all; clc;
% In this example, we use the fast proximal point method of Guler [1] for
% solving the non-smooth convex minimization problem
%   min_x F(x); for notational convenience we denote xs=argmin_x F(x);
%
% [1] O. Güler. New proximal point algorithms for convex minimization.
%               SIAM Journal on Optimization, 2(4):649–664, 1992.
% 
% We show how to compute the worst-case value of F(xN)-F(xs) when xN is
% obtained by doing N steps of the method starting with an initial
% iterate satisfying f(x0)-f(xs)+A/2*||x0-xs||^2<=1 for some A>0.
%
% Alternative interpretations:
% (1) the following code compute the solution to the problem
%       max_{F,x0,...,xN,xs} (F(xN)-F(xs))/( f(x0)-f(xs)+A/2*||x0-xs||^2 ) 
%            s.t. x1,...,xN are generated via Guler's method,
%                 F is closed, proper, and convex.
%     where the optimization variables are the iterates and the convex
%     function F.
%
% (2) the following code compute the smallest possible value of 
%     C(N, step sizes) such that the inequality
%       F(xN)-F(xs)  <= C(N, step sizes) * ( f(x0)-f(xs)+A/2*||x0-xs||^2 )
%     is valid for any closed, proper and convex F and any sequence of
%     iterates x1,...,xN generated by Guler's method on F.
%


% (0) Initialize an empty PEP
P = pep();

% (1) Set up the objective function
F = P.DeclareFunction('Convex');      % F is the objective function

% (2) Set up the starting point and initial condition
x0      = P.StartingPoint();        % x0 is some starting point
[xs,fs] = F.OptimalPoint();         % xs is an optimal point, and fs=F(xs)

% (3) Algorithm
N       = 13;		% number of iterations
A0      = 10;        % initial value of A  (see paper)
lambda  = @(k)(k/1.1);  % stepsizes (possibly function of k)  (see paper)

A   = cell(N+1,1); A{1}=A0;

x = x0;
v = x0; % second sequence of iterates (see paper)
for i = 1:N
    alpha  = (sqrt( (A{i}*lambda(i))^2 + 4 * A{i}*lambda(i) ) - A{i}*lambda(i)) / 2;
    y      = (1-alpha) * x + alpha * v;
    x      = proximal_step(y, F, lambda(i));
    v      = v + 1/alpha * (x - y);
    A{i+1} = (1-alpha) * A{i};
end
xN = x;

f0 = F.value(x0);
P.InitialCondition( f0-fs+A{1}/2*(x0-xs)^2 <= 1); % Initial condition 

% (4) Set up the performance measure
fN = F.value(xN);
P.PerformanceMetric(fN-fs);

% (5) Solve the PEP
P.solve()

% (6) Evaluate the output
% The result should be better than the theoretical guarantee from [1]:
% f(xN)-f(xs) <= 4/A{1}/(sum_{i=1}^N sqrt(lambda(i)))^2 * ( f0-fs+A{1}/2*(x0-xs)^2 )
%
% comparison:
accumulation = 0;
for i = 1:N
    accumulation=accumulation+sqrt(lambda(i));
end
theoretical_guarantee = 4/A{1}/accumulation^2;
pesto_guarantee       = double(fN-fs);
fprintf('Theoretical guarantee from [1]: f(xN)-f(xs)<= %6.5f * ( f0-fs+A{1}/2*(x0-xs)^2 )\n \t guarantee from pesto:  f(xN)-f(xs)<= %6.5f * ( f0-fs+A{1}/2*(x0-xs)^2 )\n',theoretical_guarantee,pesto_guarantee)
