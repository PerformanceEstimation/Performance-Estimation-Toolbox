clear all; clc;
% In this example, we use a Bregman proximal point method (with kernel h)
% for solving the constrained smooth strongly convex minimization problem
%   min_x { F(x) = f_1(x) + f_2(x) }
%   for notational convenience we denote xs=argmin_x F(x);
% where f_1(x) and f_2(x) are closed convex proper functions.
%
% We show how to compute the worst-case value of F(xN)-F(xs) when xN is
% obtained by doing N steps of the method starting with an initial
% iterate satisfying Dh(x*,x0)<=1. (Dh is the Bregman distance generated by
% h, between x* and x0)
%
% [1] Radu-Alexandru Dragomir, Adrien B. Taylor, Alexandre d’Aspremont, and
%     Jérôme Bolte. "Optimal Complexity and Certification of Bregman
%     First-Order Methods". (2019)

% (0) Initialize an empty PEP
P = pep();

h  = P.DeclareFunction('Convex'); % kernel of the Bregman method
f1 = P.DeclareFunction('Convex');
f2 = P.DeclareFunction('Convex');

F  = f1 + f2;
% (2) Set up the starting point and initial condition
x0        = P.StartingPoint();        % x0 is some starting point
[ xs, Fs] = F.OptimalPoint('opt');    % xs is an optimal point, and fs=F(xs)

[gh0, h0] = h.oracle(x0,'x0');
[ghs, hs] = h.oracle(xs,'opt');

P.InitialCondition( hs - h0 - gh0 * (xs - x0) <= 1); % Initial condition Dh(x*,x0)<=1

% (3) Algorithm
gamma = 3;  % stepsize
N     = 5;  % number of iterations

x = x0;
gFx = cell(N,1);
Fx  = cell(N,1);
ghx = cell(N+1,1); ghx{1} = gh0;
hx  = cell(N+1,1); hx{1}  = h0;

for i = 1:N
    name = sprintf('x%d',i);
    x    = mirror_prox(ghx{i}, h, F, gamma, name);
    
    [gFx{i}, Fx{i}]     = F.oracle(name);
    [ghx{i+1}, hx{i+1}] = h.oracle(name);
end

% (4) Set up the performance measure
P.PerformanceMetric(Fx{N}-Fs);

% (5) Solve the PEP
P.solve()

% (6) Evaluate the output
double(Fx{N}-Fs)   % worst-case objective function accuracy

% Result should match 1/gamma/N 
