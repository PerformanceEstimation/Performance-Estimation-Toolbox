\documentclass[11pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{hyperref}
\usepackage{bibentry}
\nobibliography*
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{fixltx2e}
\usepackage[framed,numbered,autolinebreaks,useliterate]{mcode}

%\usepackage{url,textcomp}

\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}

\usepackage{fancyhdr}
\pagestyle{fancy}

\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{comment}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{color}
\usepackage{pdfsync}
%\usepackage[svgnames]{xcolor}
\usepackage[tikz]{bclogo}
\usepackage{pifont}
\hypersetup{
	colorlinks   = true, %Colours links instead of ugly boxes
	urlcolor     = blue, %Colour for external hyperlinks
	linkcolor    = blue, %Colour of internal links
	citecolor   = blue %Colour of citations
}

\renewcommand{\headrulewidth}{1pt}
\newcommand{\norm}[1]{{\left\lVert#1\right\rVert}}
\newcommand{\normsq}[1]{{\left\lVert#1\right\rVert}^2}
\newcommand{\Tr}[1]{{\trace\left(#1\right)}}
\newcommand{\Rd}{\mathbb{R}^d}
\newcommand{\inner}[2]{{\langle #1, #2\rangle}}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\val}{val}
\DeclareMathOperator{\epi}{epi}
\DeclareMathOperator{\trace}{Tr}
\DeclareMathOperator{\tr}{\trace}
\DeclareMathOperator{\linspan}{span}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\FmuL}{\mathcal{F}_{\mu,L}}

\newcommand{\bv}{\mathbf{v}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bfu}{\mathbf{f}}

\newcommand{\real}{\mathbb{R}}
\fancyhead[L]{Performance Estimation Toolbox: User Manual}
\fancyhead[R]{}

\fancyfoot[C]{\textbf{page \thepage}}

\newcommand{\caution}[1]{{\color{red}{\sc Caution:} #1}}
\newcommand{\pesto}{{PESTO }}

\begin{document}
	\author{Adrien B. Taylor\footnote{INRIA, SIERRA project-team, and D.I. Ecole normale sup\'erieure, Paris, France. Email: adrien.taylor@inria.fr}, Julien M. Hendrickx\footnote{UCLouvain, ICTEAM Institute, Louvain-la-Neuve, Belgium. Email:
			julien.hendrickx@uclouvain.be}, Fran\c{c}ois Glineur\footnote{UCLouvain, ICTEAM Institute/CORE, Louvain-la-Neuve, Belgium. Email: francois.glineur@uclouvain.be}}
	\title{Performance Estimation Toolbox (PESTO): User Manual\thanks{This research is supported by the Belgian Network DYSCO (Dynamical Systems, Control, and Optimization), funded by the Interuniversity Attraction Poles Programme, initiated by the Belgian State, Science Policy Office, and of the Concerted Research Action (ARC) programme supported by the Federation Wallonia-Brussels (contract ARC 14/19-060). The scientific responsibility rests with its authors.}}
	\date{Current version: \today}
	\maketitle
	%*********
	%*********
	%		**
	%Title: **
	%		**
	%*********
	%*********

	%*********
	%*********
	%		**
	%Core:	**
	%		**
	%*********
	%*********
	\renewcommand*\contentsname{}
	\setcounter{tocdepth}{2} \tableofcontents

	\clearpage
	\section*{Foreword}
	This toolbox was written with as only objective to ease the access to the performance estimation framework for performing automated worst-case analyses. The main underlying idea is to allow the user writing the algorithms nearly as he would have implemented them, instead of performing the potentially demanding SDP modelling steps required for using the methodology.

	\section*{Contributions and feedbacks}
	In case the toolbox and/or the methodology raises some interest to you, and that you would like to provide/suggest new functionalities or improvements, we would be very happy to hear from you (also if you find any kind of typo or error). In particular, if you want to provide a PESTO example involving new methods, we would be very happy to include them within the example section along with appropriate acknowledgements.
	
	\section*{Acknowledgements}
	The authors would like to thank Fran\c{c}ois Gonze (UCLouvain), Yoel Drori (Google Inc.) and Th\'eo Golvet (ENSTA ParisTech) for their very constructive feedbacks on preliminary versions of the toolbox.
	Additional material was incorporated thanks to:
	\begin{itemize}
		\item Ernest Ryu (UCLA), Carolina Bergeling (Lund), and Pontus Giselsson (Lund) [monotone operators and splitting methods],
		\item Francis Bach (Inria \& ENS Paris) [stochastic methods, potential functions, and inexact proximal operations],
		\item Radu-Alexandru Dragomir (ENS Paris \& TSE), Alexandre d'Aspremont (CNRS \& ENS Paris), and J\'er\^ome Bolte (TSE) [Bregman divergences and new notions of smoothness].
		\item Mathieu Barr\'e (Inria \& ENS Paris), and Alexandre d'Aspremont (CNRS \& ENS Paris) [adaptive methods, and inexact proximal operations].
	\end{itemize}
	Last but not least, we thank Loic Est\`eve (Inria) for technical support.

	\section*{Recent updates}
	\begin{itemize}
		\item[05/2021] {\bf{}[Update]} Default verbose setting is medium (``2''): PESTO in verbose mode, but solver not.
		\item[05/2021] {\bf{}[Update]} Update in the \verb|PerformanceMetric| command, which allows replacing previous performance measure (instead of taking the $\min$, which it still does by default, see example in Section~\ref{sec:basicuse}).
		\item[05/2021] {\bf{}[Update]} We can add names to additional (custom) constraints, which now appear in the corresponding list of dual variables (along with their names).
		\item[05/2021] {\bf{}[New]} Possibility of using additional linear matrix inequalities (LMIs) using the new command \verb?AddLMIConstraint?.
		\item[05/2021] {\bf{}[New]} Decentralized Gradient Descent (DGD) added to the examples.
		\item[05/2020] {\bf{}[New]} New primitives and examples: inexact proximal operations.
		\item[02/2020] {\bf{}[New]} Example section extended with two adaptive methods (variants of Polyak steps).
		\item[11/2019] {\bf{}[New]} The example section was largely extended and now contain about {\bf 50~examples}. Newcomers include:  stochastic methods, non-Euclidean methods (e.g., NoLips), nonconvex applications and monotone inclusions. We also show how to use the toolbox to verify potential functions.
		\item[11/2019] {\bf{}[New]} New primitives: mirror and proximal mirror steps.
		\item[11/2019] {\bf{}[Update]} Updated operations on functions: additions, subtractions, and scalings are implemented in a more efficient fashion.
		\item[11/2019] {\bf{}[Update]} Behaviors of tags was slightly modified (see late Section~\ref{sec:tags}), allowing more straightforward implementations of algorithms such as Bregman gradient (a.k.a., NoLips) methods.
		\item[11/2019] {\bf{}[Correction]}: Proximal operations for sums of functions were not effectively implemented.
		\item[12/2018] {\bf{}[New]} Maximally monotone, Lipschitz, cocoercive and strongly monotone operators were added (see Section~\ref{sec:operators}). Note that \verb|PESTO| was initially thought for handling performance of optimization algorithms. Internally, the toolbox handles operators exactly as it does with function, except that no function values are available for operators.
		\item[12/2018] {\bf{}[New]} Example section completed with much more material. In particular, we added applications to operator splitting methods and fixed-point iterations.
	\end{itemize}
	
	\section*{Contributors}
	\begin{itemize}
		\item Adrien Taylor (\href{http://www.di.ens.fr/~ataylor/}{personal page})
		\item Julien Hendrickx (\href{https://perso.uclouvain.be/julien.hendrickx/index.html}{personal page})
		\item Fran\c{c}ois Glineur (\href{https://perso.uclouvain.be/francois.glineur/}{personal page})
		\item S\'ebastien Colla (\href{https://perso.uclouvain.be/sebastien.colla/}{personal page})
	\end{itemize}

	%\section*{Updates plan}
	%%%%======================================
	%%%%									%||
	%%%\section{}		%||
	%%%%==============================		%||
	%%%%======================================
	%\subsection{Upcoming tools}
	%The PESTO toolbox is still under developments
	%%%%\subsection{Recovering dual variables}
	%%%\begin{itemize}
	%%%\item {\color{red}\sc (Upcoming)} dual variable recovery and proof helper
	%%%\item {\color{red}\sc (Upcoming)} proof helper
	%%%\item {\color{red}\sc (Upcoming)} worst-case function recovery and interpolation
	%%%\item {\color{red}\sc (Upcoming)} GFOM-procedure with separable structure?
	%%%\end{itemize}
	%\subsection{Simplifying the PEP via relaxations}
	%\subsection{Recovering discrete function}
	\clearpage
	%==================================
	%								%||
	\section{Introduction}			%||
	%============================	%||
	%==================================

	This note details the working procedure of the performance estimation toolbox, whose aim is to ease and improve the performance analyses of first-order optimization methods. The methodology originates from the seminal work on performance estimation of Drori and Teboulle~\cite{Article:Drori} (see also \cite{drori2014contributions} for a full picture of the original developments), and on the subsequent convex interpolation framework developed by the authors~\cite{taylor2015smooth,taylor2015exact} for obtaining non-improvable guarantees for families of first-order methods and problem classes. The interested readers can find a complete survey on the performance estimation literature in the recent~\cite{Taylor2017PEPs}.

	The performance estimation toolbox relies on the use of the \textsc{Yalmip}~\cite{Article:Yalmip} modelling language within \textsc{Matlab}, and on the use of an appropriate semidefinite programming (SDP) solver (see for example~\cite{Article:Sedumi,Article:Mosek,Article:sdpt3}). Note that the toolbox is not intended to provide the most efficient implementation of the performance estimation methodology, but rather to provide a simple, generic and direct way to use it. In addition, it is important to have in mind that our capability to (accurately) solve PEPs is inherently limited to our capability to solve semidefinite programs. Typically, the methodology is well-suited for studying a few iterations of simple optimization schemes, but its computational cost may become prohibitive in the case of a large number of iterations (see examples below).

	\begin{enumerate}
		\item Please reference \pesto when used in a published work:
		\begin{itemize}
			\item \bibentry{pesto2017}
		\end{itemize}
		Note that the general methodology used in \pesto is presented in the following works:
		\begin{itemize}
			\item \bibentry{taylor2015smooth}
			\item \bibentry{taylor2015exact}
		\end{itemize}
		\item We distribute \pesto  for helping researchers of the field, but we do not provide any warranty on the provided results. In particular, note that:
		\begin{itemize}
			\item our capability to solve performance estimation problems is limited by our capability to solve semidefinite programs. Therefore, the solver choice is of utmost importance, as well as an appropriate treatment of the errors/numerical problems within the solver. Good practices regarding the use of the toolbox are presented in Section~\ref{sec:basicuse}.
			\item The toolbox is not aimed to provide computationally efficient implementations of the PEPs. It is foremost designed for (1) obtaining preliminary results on the worst-case performance of simple optimization schemes, (2) helping researchers obtaining worst-case guarantees on their algorithms, and (3) providing a simple numerical validation tool for assessing the quality of other analytical or numerical worst-case guarantees.
		\end{itemize}
	\end{enumerate}

	Depending on the final goal, the advanced users may prefer develop their own (optimized) codes for studying specific algorithms.
	\paragraph{Related methodology} Semidefinite programming was also used in a related approach~\cite{lessard2014analysis} for obtaining bounds on the worst-case guarantees. This alternative approach is specialized for obtaining asymptotic linear rates of convergence and relies on control theory via the so-called \emph{integral quadratic constraints} (IQC) framework. The relation between performance estimation and integral quadratic constraints for studying performances of first-order optimization schemes was recently showed in~\cite{taylor2018lyapunov}. In a few words,~\cite{taylor2018lyapunov} formulates performance estimation problems for looking towards \emph{Lyapunov functions} (i.e., compact proofs of linear convergence).

	\clearpage
	%======================================
	%									%||
	\section{Setting up the toolbox}		%||
	%==============================		%||
	%======================================


	\paragraph{Pre-requisites} In order to install the package, please make sure that both \href{https://yalmip.github.io/}{\textsc{Yalmip}} (Version 19-Sep-2015 or later) and some SDP solver (e.g., \href{http://sedumi.ie.lehigh.edu/}{SeDuMi}~\cite{Article:Sedumi}, \href{https://mosek.com/}{MOSEK}~\cite{Article:Mosek}, or~\href{http://www.math.nus.edu.sg/~mattohkc/sdpt3.html}{SDPT3}~\cite{Article:sdpt3}) are installed and properly working on your computer. For testing the proper installation of \textsc{Yalmip} and a SDP solver, you may run the following command
	\begin{verbatim}
	>> yalmiptest
	\end{verbatim}
	\paragraph{Downloading the code} The toolbox is fully available from the following {\sc Github} repository: \begin{center}
		\href{https://github.com/AdrienTaylor/Performance-Estimation-Toolbox}{\sc AdrienTaylor/Performance-Estimation-Toolbox}.\\
	\end{center}

	\paragraph{Install \pesto}
	\begin{verbatim}
	>> Install_PESTO
	\end{verbatim}

	\paragraph{First aid within \pesto}
	\begin{verbatim}
	>> help pesto
	\end{verbatim}
	Further support can be obtained by contacting the authors.

	The best way to quickly get used to the framework is by probably by using the different demonstration files that are available within PESTO. Available demos are summarized by typing:
	\begin{verbatim}
	>> demo
	\end{verbatim}

	\clearpage

	%======================================
	%									%||
	\section{Basic use of the toolbox}	%||
	%==============================		%||
	%======================================
	\label{sec:basicuse}

	For the complete pictures and details on the approach, we refer to~\cite[Section 1\&3]{taylor2015smooth} for the simplified case for smooth strongly convex unconstrained minimization, and to~\cite[Section 1\&2]{taylor2015exact} for the full approach for taking into account non-smooth, constrained, composite or finite sums terms in the objective function, with first-order methods possibly involving projection, linear-optimization, proximal or inexact operations. For the sake of simplicity, we approach the toolbox via an example, allowing to go through the different important elements to consider.

	Let us consider the following non-smooth convex minimization problem
	\begin{equation}
	\min_{x\in\Rd} f(x),\tag{OPT}\label{eq:origOpt}
	\end{equation}
	with $f$ being a closed, convex and proper function with bounded subgradients, i.e., for all $g\in\partial f(x)$ for some $x\in\Rd$, we have $\norm{g}\leq R$ for some constant $R\geq 0$ (for convenience, we denote $f\in \mathcal{C}_R$).
	In this example, we study the worst-case performance of the projected subgradient method for solving~\eqref{eq:origOpt}:
	\begin{equation}
	x_{i}=x_{i-1}-h_{i-1}f'(x_{i-1}),\label{eq:subgrad}
	\end{equation}
	where  $f'(x_{i-1})\in\partial f(x_{i-1})$ is a subgradient of $f$ at $x_{i-1}$, and $h_i\in\real$ is some step size parameter. For the worst-case performance measure, we chose to use the criterion \[\min_{0\leq i\leq N} f(x_i)-f(x_\star),\]
	with $x_\star$ an optimal solution to~\eqref{eq:origOpt}, and $N$ being the number of iterations. Finally, in order to have a bound worst-case measure, we need to consider an \emph{initial condition}; we chose to consider that the initial iterate $x_0$ to satisfy the following quality measure:
	\[ \norm{x_0-x_\star}\leq 1,\] with the initial distance being arbitrarily set to $1$ (see homogeneity relations~\cite[Section 3.5]{taylor2015smooth}).

	\subsection{Performance estimation problems}
	The key idea underlying the performance estimation approach relies in using the definition of the \emph{worst-case behavior}. That is, the worst-case behavior of
	\begin{align}
	&\max_{f,\{x_i\},x_\star} \ \min_{0\leq i\leq N} f(x_i)-f(x_\star),  \tag{PEP$(d)$}\label{Intro:PEP} \\
	&\quad\quad \text{s.t. } f\in \mathcal{C}_R(\Rd)  \notag\\
	&\quad\quad\quad\quad    x_0 \text{ satisfies some initialization conditions: } \normsq{x_0-x_\star}\leq 1\notag \\
	&\quad\quad\quad\quad    x_{i} \text{ is computed by~\eqref{eq:subgrad} for all $1 \le i \le N$,}
	\notag\\
	&\quad\quad\quad\quad    x_\star \text{ is a minimizer of } f(x). \notag
	\end{align}
	For treating~\eqref{Intro:PEP}, we use semidefinite programming. All the modelling steps for going from~\eqref{Intro:PEP} to a semidefinite program for more complicated settings are detailed in~\cite{taylor2015smooth} and~\cite{taylor2015exact}.

	The first step taken in that direction is to use a discrete version of~\eqref{Intro:PEP}, replacing the \emph{infinite-dimensional} variable and constraint $f\in \mathcal{C}_R(\Rd)$ by an \emph{interpolation constraint} using only coordinates $x_i$, subgradients $g_i$ and function values $f_i$ of the iterates and of an optimal point:
	\[\exists f\in\mathcal{C}_R:\ g_i\in\partial f(x_i) \text{ and } f_i=f(x_i) \ \text{for all } i\in I \overset{\text{(Definition)}}{\Leftrightarrow} \{(x_i,g_i,f_i)\}_{i\in I} \text{ is }\mathcal{C}_R\text{-interpolable},\] with $I=\{0,1,\hdots,N,*\}$.
	One can show that this constraint can equivalently be formulated as\footnote{
		Interpolation conditions for other classes of functions can be found in~~\cite[Section 3]{taylor2015exact}.}
	\[ \{(x_i,g_i,f_i)\}_{i\in I} \text{ is }\mathcal{C}_R\text{-interpolable} \Leftrightarrow f_i\geq f_j+\inner{g_j}{x_i-x_j}\text{ for all } i,j\in I\text{, and } \normsq{g_i}\leq R^2 \text{ for all }i\in I.\]
	Therefore, assuming without loss of generality that $x_\star=g_\star=0$ and that $f_\star=0$, the problem~\eqref{Intro:PEP} can be reformulated as
	\begin{align}
	&\max_{\{x_i,g_i,f_i\}_{i\in I}} \ \min_{0\leq i\leq N} f_i,  \tag{discrete-PEP$(d)$}\label{Intro:PEP2} \\
	&\quad\quad \text{s.t. } f_i\geq f_j+\inner{g_j}{x_i-x_j}\text{ for all } i,j\in I\text{, and } \normsq{g_i}\leq R^2 \text{ for all }i\in I,  \notag\\
	&\quad\quad\quad\quad    x_i,g_i\in\Rd \text{ for all }i\in I,\notag\\
	&\quad\quad\quad\quad    x_0 \text{ satisfies some initialization conditions: } \normsq{x_0-x_\star}\leq 1\notag, \\
	&\quad\quad\quad\quad    x_{i}=x_{i-1}-h_{i-1}g_{i-1} \text{ for all } 0 \le i \le N-1,
	\notag,\\
	&\quad\quad\quad\quad    g_\star=0. \notag
	\end{align}
	For solving~\eqref{Intro:PEP2}, we introduce the following notations:
	\[P=[g_0 \ g_1 \ \hdots \ g_N \ x_0],\]
	along with $\bg_i=e_{1+i}$ (for $i=0,\hdots,N$), $\bx_0=e_{N+2}$, $\bx_{i}=\bx_{i-1}-h_{i-1}\bg_{i-1}$ (for $i=1,\hdots,N$) and $\bg_\star=\bx_\star=0$. Those notations allow conveniently writing for all $i\in\{0,1,\hdots,N,*\}$
	\begin{equation*}
	\begin{aligned}
	x_i&=P\bx_i,\\
	g_i&=P\bg_i.
	\end{aligned}
	\end{equation*}
	Using the previous notations, we note that all scalars products and norms present in the formulation~\eqref{Intro:PEP2} can be written by combining entries of the matrix $G=P^{\top\!}P$ which is positive semidefinite by construction (notation $G\succeq 0$). Indeed, we have the following equalities:
	\[\inner{g_j}{x_i-x_j}=\bg_i^{\top\!} G(\bx_i-\bx_j),\quad \normsq{x_0-x_\star}=\bx_0^{\top\!} G\bx_0, \text{and } \normsq{g_i}=\bg_i^{\top\!} G\bg_i.\]
	In addition, we have the following equivalence:
	\[ G\succeq 0, \ \mathrm{rank}\ G\leq d \Leftrightarrow G=P^\top P \text{ with } P\in\mathbb{R}^{d\times (N+2)},\]
	which allows writing~\eqref{Intro:PEP2} as a rank-constrained semidefinite program (SDP).
	\begin{align}
	&\max_{G\succeq 0, \{f_i\}_{i=0,\hdots,N},\tau} \ \tau,  \tag{SDP-PEP$(d)$}\label{Intro:PEP3} \\
	&\quad\quad \text{s.t. } f_i\geq f_j+\inner{g_j}{x_i-x_j}\text{ for all } i,j\in I\text{, and } \normsq{g_i}\leq R^2 \text{ for all }i\in I,  \notag\\
	&\quad\quad\quad\quad    \tau \leq f_i\text{ for all }i\in I,\notag\\
	&\quad\quad\quad\quad    \normsq{x_0-x_\star}\leq 1\notag,\\
	&\quad\quad\quad\quad \rank{G}\leq d\notag.
	\end{align}
	For obtaining a formulation that is both \emph{tractable} and \emph{valid for all dimensions}, one can relax the rank constraint from~\eqref{Intro:PEP3}, and solve the corresponding simplified SDP. That is, instead of considering solving~\eqref{Intro:PEP3} for all values of $d$, we solve~\eqref{Intro:PEP3} only for $d=N+2$ (see~\cite[Remark 3]{taylor2015exact}). The worst-case guarantee obtained by solving (PEP$(N+2)$) is valid for any value of the dimension parameter $d$, and is guaranteed to be \emph{exact} (i.e., or non-improvable) as long as $d\geq N+2$ (the so-called \emph{large-scale} assumption). In addition, it can be solved using standard SDP solvers such as~\cite{Article:Sedumi,Article:Mosek,Article:sdpt3}.\vspace{1cm}
	\newpage
	\begin{bclogo}[logo=\bcattention, couleur=blue!30, arrondi =0.1, sousTitre=rank deflection constraints]{Good practice}
		Due to current techniques for solving semidefinite programs, the presence of constraints enforcing \emph{rank-deficiency} of the Gram matrix may critically deteriorate the quality of the numerical solutions. For avoiding that, the user should evaluate as few function values and gradients as possible (for limiting the size of the Gram matrix), avoid replicates (avoid evaluating two times the same gradient at the same point), and generally avoid constraints enforcing linear dependence between two vectors. Common examples include
		\begin{itemize}
			\item (algorithmic constraints imposing rank-deficiency) a constraint $\norm{x_1-x_0+f'(x_0)}^2=0$ enforces the equality $x_1=x_0-f'(x_0)$. You should instead consider substituting $x_1$ by $x_0-f'(x_0)$; this is done automatically by the toolbox by defining $x_1$ as $x_0-f'(x_0)$ (see example from Section~\ref{ex:gm_steps}).
			\item (interpolation constraints imposing rank-deficiency) When performing several subgradient evaluations at the same point, the corresponding subgradients may in general be different (if the subdifferential is not a singleton). However, if you evaluate several times the gradient of a differentiable function at the same point, smoothness \emph{implicitly} imposes a rank deficiency on the Gram matrix, as it is equivalent to $\norm{g_1-g_2}^2=0$, where $g_1,g_2\in\partial f(x)$.
			\item (inexactness model imposing rank-deficiency --- see examples and~\cite{deKlerkELS2016}) Consider an initial iterate $x_0$ and a search direction given by a vector $d_0$ satisfying a relative accuracy criterion: $\norm{d_0-f'(x_0)}\leq \varepsilon\norm{f'(x_0)}$. In the case $\varepsilon=0$, the model imposes
			$\norm{d_0-f'(x_0)}\leq 0$ and hence $d_0=f'(x_0)$. It is far better to consider substituting $d_0$ by $f'(x_0)$ instead of using the constraint $\norm{d_0-f'(x_0)}^2=0$.
		\end{itemize}
	\end{bclogo}
	\newpage
	\subsection{Setting up the PEP within PESTO: full example}\label{ex:gm_steps}

	In this section, we exemplify the approach for studying $N$ steps of a subgradient method for minimizing a convex function with bounded subgradients. We chose to use the constant step size rule $h_i=\frac{1}{\sqrt{N+1}}$ and arbitrarily consider the class $\mathcal{C}_R$ with $R=1$. The example is detailed in the following sections.
\begin{lstlisting}
% (0) Initialize an empty PEP
P=pep();

% (1) Set up the objective function
param.R=1;	% 'radius'-type constraint on the subgradient norms: ||g||<=1

% F is the objective function
F=P.DeclareFunction('ConvexBoundedGradient',param);

% (2) Set up the starting point and initial condition
x0=P.StartingPoint();            % x0 is some starting point
[xs,fs]=F.OptimalPoint();        % xs is an optimal point, and fs=F(xs)
P.InitialCondition((x0-xs)^2<=1); % Add an initial condition ||x0-xs||^2<= 1

% (3) Algorithm and (4) performance measure
N=5; % number of iterations
h=ones(N,1)*1/sqrt(N+1); % step sizes

x=x0;

% Note: the worst-case performance measure used in the PEP is the
%       min_i (PerformanceMetric_i) (i.e., the best value among all
%       performance metrics added into the problem. Here, we use it
%       in order to find the worst-case value for min_i [F(x_i)-F(xs)]

% we create an array to save all function values (so that we can evaluate
% them afterwards)
f_saved=cell(N+1,1);
for i=1:N
	[g,f]=F.oracle(x);
	f_saved{i}=f;
	P.PerformanceMetric(f-fs);
	x=x-h(i)*g;
end

[g,f]=F.oracle(x);
f_saved{N+1}=f;
P.PerformanceMetric(f-fs);

% (5) Solve the PEP
P.solve();

% (6) Evaluate the output
for i=1:N+1
	f_saved{i}=double(f_saved{i});
end
f_saved
% The result should be (and is) 1/sqrt(N+1).
	\end{lstlisting}
	\newpage
	\subsection{Basic objects and algebraic operations}\label{sec:basicobjects}
	There are four essential types of objects implemented within the toolbox:
	\begin{enumerate}
		\item functions, for which we refer to~\ref{sec:functions}. Functions can be created, added, subtracted, scaled, and evaluated. There are two basic ways to create functions: firstly, by relying on the \verb?DeclareFunction? method of a PEP object (see Section~\ref{ex:gm_steps}, step (0) Initialization of a PEP), and secondly by performing basic operations with other functions (sums, differences scaling, etc.). In the following example, we create and add two convex functions (more functional classes are described in the sequel).\\[-1cm]
		\begin{lstlisting}
% We declare two convex functions: f1 and f2.
f1=P.DeclareFunction('Convex');
f2=P.DeclareFunction('Convex');

% We create a new function F that is the sum of f1 and f2.
F=f1+f2;
		\end{lstlisting}
		Let $\verb?x0?$ be some initial point. In order to evaluate the function, there are three standard ways. First, if only the subgradient of $F$ at $x_0$ is of interest, one can use the following.\\[-1cm]
		\begin{lstlisting}
% Evaluating a subgradient of F at x0.
g0=F.subgradient(x0);
		\end{lstlisting} If only the function value $F(x_0)$ is of interest, one can use the following alternative.\\[-1cm]
		\begin{lstlisting}
% Evaluating  F(x0).
F0=F.value(x0);
		\end{lstlisting} Finally, if both a subgradient and a function value are of interest, we advise the user to use the following construction (which is better than combining the previous ones) performing both evaluations simultaneously.\\[-1cm]
		\begin{lstlisting}
% Evaluating  F(x0) and a subgradient of F at x0.
[g0,F0]=F.oracle(x0);
		\end{lstlisting}
		\item Vectors, which can be created, added, subtracted or multiplied (inner product) with each other or with a constant (also, divisions by nonzero constants are accepted). Once the PEP object is solved, vectors can also be evaluated. The basic operations for creating a vector are the following
		\begin{itemize}
			\item by evaluating a subgradient of a function (see previous point),
			\item by generating a \emph{starting point}, that is, generating a point with no constraint (yet) on its position. This operation can be repeated to generate as much starting points as needed.\\[-1cm]
			\begin{lstlisting}
x0=P.StartingPoint(); % x0 is some starting point
			\end{lstlisting}
			\item Also, it is possible to generate an optimal point of a given function.\\[-1cm]
			\begin{lstlisting}
[xs,fs]=F.OptimalPoint(); % xs is an optimal point, and fs=F(xs)
			\end{lstlisting}
			\item Finally, it is possible to define new vectors by combining other ones. For example, for describing the iterations of an algorithm.\\[-1cm]
			\begin{lstlisting}
x=x-F.subgradient(x); % subgradient step (step size 1)
			\end{lstlisting}
			Note an alternate form for the previous code is as follows\\[-1cm]
			\begin{lstlisting}
x=gradient_step(x,F,1); % subgradient step (step size 1)
			\end{lstlisting}
		\end{itemize}
		It is also possible to compute inner products of pairs of vectors, resulting in scalar values. This operation is essential for defining (among others) initial conditions, performance measures, and interpolation conditions.\\[-1cm]
		\begin{lstlisting}
% scalar_value1 is squared distance between x and the optimal point xs.
scalar_value1=(x-xs)^2;

% scalar_value2 is the inner product of a subgradient of F at x and x
scalar_value2=F.subgradient(x)*x;
		\end{lstlisting}
		Finally, once the corresponding PEP has been solved, vectors (and scalars) involved in this PEP can be evaluated using the \verb?double? command. For example, the following evaluations are valid:\\[-1cm]
		\begin{lstlisting}
double(scalar_value1), double(x0-xs), double((x0-xs)^2), double(scalar_value2)
		\end{lstlisting}
		\item Scalars (constants, function values $f_i$'s or inner products $\inner{g_i}{x_i}$'s), which can be added, subtracted with each others (also, divisions by nonzero constants are accepted). Scalars can also be used to generate constraints (see next point). Once the PEP object is solved, scalars can also be evaluated.
		\item Constraints (see also Section~\ref{sec:constraints}), which can be created by linearly combining scalar values in an (in)equality. In addition to the interpolation constraints, the two most common examples involve initial conditions and performance measures. The following code is valid and add two \emph{initialization} constraints to the PEP:\\[-1cm]
		\begin{lstlisting}
P.InitialCondition((x0-xs)^2<=1);% Add an initial condition ||x0-xs||^2<= 1
P.InitialCondition(F0-Fs<=1);	% Add an initial condition F0-Fs<= 1
		\end{lstlisting}
		In PESTO, the performance measure is assumed to be of the form $\min_k \{m_k(G,\{f_i\}_i)\}$, where each $m_k(.)$ is a performance measure (e.g., in the previous subgradient example, we used $\min_{0\leq k \leq N} f_k-f_\star$). The command \verb?PerformanceMetric? allows to create new $m_k(.)$'s.\\[-1cm]
		\begin{lstlisting}
P.PerformanceMetric((x-xs)^2); 		% Add a performance measure ||x-xs||^2
P.PerformanceMetric(F.value(x)-Fs);	% Add a performance measure F(x)-Fs
		\end{lstlisting}
		Note that as in the example~\eqref{Intro:PEP3}, the performance metrics are encoded as new constraints involving the objective function $\tau$. The default behavior of \verb?PerformanceMetric? can also be changed to replace the previous objectives, as follows.
		\begin{lstlisting}
P.PerformanceMetric((x-xs)^2); 	% Add a performance measure ||x-xs||^2
P.PerformanceMetric((F.gradient(x))^2,'min'); % Add a performance measure ||F'(x)||^2, same behavior as P.PerformanceMetric((F.gradient(x))^2)
% At this point, the performance measure is min{ ||x-xs||^2, ||F'(x)||^2}.
% Let's change that to F(x)-Fs.
P.PerformanceMetric(F.value(x)-Fs,'replace');	% Replace all previously defined performance measures by F(x)-Fs
		\end{lstlisting}
	\end{enumerate}
	\newpage
	\subsection{Functional classes} \label{sec:functions}
	In PESTO, interpolation constraints are hidden to the users, and are specifically handled by routines in the \verb?Functions_classes? directory. The list of functional classes for which interpolation constraints are handled in the toolbox is presented in Table~\ref{Tab:func_classes}.  For details about the corresponding input parameters, type \verb?help ClassName? in the Matlab prompt (e.g., \verb?help Convex?).
	\begin{table}[ht!]
		{
			\begin{center}
				{\renewcommand{\arraystretch}{1.2}
					\begin{tabular}{@{}llc@{}}
						\specialrule{2pt}{1pt}{1pt}
						Function class & PESTO routine name  & Tightness\\
						\hline
						Convex functions &  \verb?Convex? &\ding{52}\\
						Convex functions (bounded subdifferentials) &  \verb?ConvexBoundedGradient? &\ding{52}\\
						Convex indicator functions  (bounded domain) & \verb?ConvexIndicator? &\ding{52}\\
						Convex support functions (bounded subdifferentials)  &  \verb?ConvexSupport?&\ding{52}\\
						Smooth strongly convex functions   & \verb?SmoothStronglyConvex? &\ding{52}\\
						Smooth (possibly nonconvex) functions &  \verb?Smooth?&\ding{52}\\
						Smooth convex functions  (bounded subdifferentials) &  \verb?SmoothConvexBoundedGradient?&\ding{52}\\
						Strongly convex functions (bounded domain) & \verb?StronglyConvexBoundedDomain?&\ding{52}\\
						\specialrule{2pt}{1pt}{1pt}
					\end{tabular}
					\caption{Default functional classes within PESTO. Some classes are overlapping and are present only for promoting a better readability of the code. The corresponding interpolation conditions are developed in~\cite[Section 3.1]{taylor2015exact}.}
					\label{Tab:func_classes}}
			\end{center}}
		\end{table}


		\begin{bclogo}[logo=\bcattention, couleur=blue!30, arrondi =0.1, sousTitre=Interpolation and hidden assumptions]{Good to know}
			The functions are only required to be interpolated at the points they were evaluated. This conception is of utmost importance when performing PEP-based worst-case analyses, as this may incorporate \emph{hidden assumptions}. Common examples include:
			\begin{itemize}
				\item (existence of optimal point) not evaluating the function at an optimal point is equivalent not to assume the existence of an optimal point. Hence, the worst-case guarantees will be valid even when no optimal point exists.
				\item (feasible initial point) In the case of constrained minimization, not evaluating the corresponding indicator function at an initial point is equivalent not to assume that this point is feasible (i.e., we do not require the existence of a subgradient of the indicator function at that point). Hence, the worst-case guarantees will be valid even for  infeasible initial points.
			\end{itemize}
			Note that those remark are also generically valid when performing convergence proofs. As PEPs can be seen as black-boxes proof generator, it is of utmost importance to be aware of the assumptions being made.
		\end{bclogo}
		\vspace{1cm}
		As an illustration of the previous remark, the following codes can be used to study the worst-case performances of the projected gradient method for minimizing a (constrained) smooth strongly convex function. In the first case, we require $x_0$ to be feasible, by evaluating the indicator function at $x_0$ (i.e., we require the indicator function to have a subgradient at $x_0$, and hence force $x_0$ to be feasible).
		\newpage
		\paragraph{Example 1} In this example, $x_0$ is feasible.
		\begin{lstlisting}
% In this example, we use a projected gradient method for
% solving the constrained smooth strongly convex minimization problem
%   min_x F(x)=f_1(x)+f_2(x);
%   for notational convenience we denote xs=argmin_x F(x);
% where f_1(x) is L-smooth and mu-strongly convex and where f_2(x) is
% a convex indicator function.
%
% We show how to compute the worst-case value of F(xN)-F(xs) when xN is
% obtained by doing N steps of the method starting with an initial
% iterate satisfying F(x0)-F(xs)<=1.

% (0) Initialize an empty PEP
P=pep();

% (1) Set up the objective function
paramf1.mu=.1;	% Strong convexity parameter
paramf1.L=1;    % Smoothness parameter
f1=P.DeclareFunction('SmoothStronglyConvex',paramf1);
f2=P.DeclareFunction('ConvexIndicator');
F=f1+f2; % F is the objective function

% (2) Set up the starting point and initial condition
x0=P.StartingPoint();		 % x0 is some starting point
[xs,fs]=F.OptimalPoint(); 	 % xs is an optimal point, and fs=F(xs)
[g0,f0]=F.oracle(x0);

P.InitialCondition(f0-fs<=1); % Add an initial condition f0-fs<=1

% (3) Algorithm
gam=1/paramf1.L;		% step size
N=1;		% number of iterations

x=x0;
for i=1:N
	xint=gradient_step(x,f1,gam);
	x=projection_step(xint,f2);
end
xN=x;
fN=F.value(xN);

% (4) Set up the performance measure
P.PerformanceMetric(fN-fs);

% (5) Solve the PEP
P.solve()

% (6) Evaluate the output
double(fN-fs)   % worst-case objective function accuracy

% Result should be (and is) max((1-paramf1.mu*gam)^2,(1-paramf1.L*gam)^2)
		\end{lstlisting}
		\newpage
		\paragraph{Example 2} In this example, $x_0$ is not required to be feasible.

		\begin{lstlisting}
% In this example, we use a projected gradient method for
% solving the constrained smooth strongly convex minimization problem
%   min_x F(x)=f_1(x)+f_2(x);
%   for notational convenience we denote xs=argmin_x F(x);
% where f_1(x) is L-smooth and mu-strongly convex and where f_2(x) is
% a convex indicator function.
%
% We show how to compute the worst-case value of F(xN)-F(xs) when xN is
% obtained by doing N steps of the method starting with an initial
% iterate satisfying ||x0-xs||^2<=1.

% (0) Initialize an empty PEP
P=pep();

% (1) Set up the objective function
paramf1.mu=.1;	% Strong convexity parameter
paramf1.L=1;    % Smoothness parameter
f1=P.DeclareFunction('SmoothStronglyConvex',paramf1);
f2=P.DeclareFunction('ConvexIndicator');
F=f1+f2; % F is the objective function

% (2) Set up the starting point and initial condition
x0=P.StartingPoint();		 % x0 is some starting point
[xs,fs]=F.OptimalPoint(); 	 % xs is an optimal point, and fs=F(xs)

P.InitialCondition((x0-xs)^2<=1); % Add an initial condition ||x0-xs||^2<= 1

% (3) Algorithm
gam=1/paramf1.L;		% step size
N=1;		% number of iterations

x=x0;
for i=1:N
	xint=gradient_step(x,f1,gam);
	x=projection_step(xint,f2);
end
xN=x;
fN=F.value(xN);

% (4) Set up the performance measure
P.PerformanceMetric(fN-fs);

% (5) Solve the PEP
P.solve()

% (6) Evaluate the output
double(fN-fs)   % worst-case objective function accuracy

% Result should be (and is) max((1-paramf1.mu*gam)^2,(1-paramf1.L*gam)^2)
		\end{lstlisting}
		\newpage
		\subsection{First-order information recovery}\label{sec:oracles}
		As for interpolation conditions, the different models for first-order information recovery (oracles) are hidden to the users, and are specifically handled by routines within the \verb?Primitive_oracles? directory. There are essentially two types of oracles available at the moment, which are summarized in Table~\ref{Tab:prim_oracles}.  For details about the corresponding input parameters, type \verb?help OracleName? in the Matlab prompt (e.g., \verb?help subgradient?).

		\begin{table}[ht!]{
				\begin{center}
					{\renewcommand{\arraystretch}{1.2}
						\begin{tabular}{@{}llc@{}}
							\specialrule{2pt}{1pt}{1pt}
							Type  & PESTO routine name &Tightness \\
							\hline
							Gradient/subgradient & \verb?subgradient? & \ding{52}\\
							Inexact gradient/subgradient (relative inaccuracy)& \verb?inexactsubgradient? & \ding{52}\\
							Inexact gradient/subgradient (absolute inaccuracy)& \verb?inexactsubgradient? & \ding{52}\\
							\specialrule{2pt}{1pt}{1pt}
						\end{tabular}
						\caption{First-order information recovery within PESTO.}
						\label{Tab:prim_oracles}}
				\end{center}}
			\end{table}


			\subsection{Standard algorithmic steps}\label{sec:alg_steps}
			In the same philosophy as for functional classes (Section~\ref{sec:functions}) and oracles (Section~\ref{sec:oracles}), the implementation of several standard algorithmic operations are hidden to the users, and are handled by routines within the \verb?Primitive_steps? directory. The list of primitive algorithmic operations is presented in Table~\ref{Tab:prim_algorithmic_steps}. For details about the corresponding input parameters, type \verb?help StepName? in the Matlab prompt (e.g., \verb?help gradient_step?).
			\begin{table}[ht!]{
					\begin{center}
						{\renewcommand{\arraystretch}{1.2}
							\begin{tabular}{@{}llc@{}}
								\specialrule{2pt}{1pt}{1pt}
								Algorithmic step  & \pesto routine name & Tightness\\
								\hline
								Gradient/subradient step & \verb?gradient_step? & \ding{52}\\
								Projection step & \verb?projection_step? & \ding{52}\\
								Proximal step & \verb?proximal_step? & \ding{52}\\
								Conditional/Frank-Wolfe/linear optimization step & \verb?linearoptimization_step? & \ding{52}\\
								Line search & \verb?exactlinesearch_step? & \ding{54}\\
								Inexact proximal step & \verb?inexact_proximal_step? & \ding{52}\\
								{Mirror/Bregman step} & \verb?mirror? & \ding{52}\\
								{Proximal mirror/Bregman step} & \verb?mirror_prox? & \ding{52}\\
								\specialrule{2pt}{1pt}{1pt}
							\end{tabular}
							\caption{Standard algorithmic steps within PESTO. Note that no tightness guarantees are provided when using exact line searches (i.e., the code will only provide upper bounds in that case, which are often tight but probably not always~\cite{drori2018efficient,deKlerkELS2016}).}
							\label{Tab:prim_algorithmic_steps}}
					\end{center}}
				\end{table}

				\subsection{Solving the PEP}
				By default \verb|pep.solve| will display the PEP (SDP) size and the solver output.
				Verbosity can be controlled using the \verb?verbose_pet? argument:
				\begin{lstlisting}
P.solve(0) %for no output
P.solve(1) %for default display
P.solve(2) %for detailed display
				\end{lstlisting}
				Without additional specifications, \verb|pep.solve| calls the default Yalmip solver with no output.
				Changing the solver used and its display level can be done with an extra \verb|solver_opt| argument (see \href{https://yalmip.github.io/command/sdpsettings/}{Yalmip's guide} for \verb|sdpsetting|):
				\begin{lstlisting}
P.solve(1,sdpsettings('solver','SeDuMi')) %to use SeDuMi with default display
P.solve(1,sdpsettings('solver','mosek','mosek.MSK_DPAR_INTPNT_CO_TOL_PFEAS',1e-10)) %to use MOSEK with extra accuracy
				\end{lstlisting}
				\subsection{Obtaining low-dimensional worst-case examples}
				The toolbox is featured with an additional option for trying to enforce the worst-case examples to be \emph{as low-dimensional} as possible.
				This is done via a \emph{trace heuristic}: using this option, the performance estimation problem is solved \emph{twice}:
				\begin{enumerate}
					\item firstly, for computing the worst-case value.
					\item Secondly, by imposing the value of the performance measure to be equal to the worst-case value and by minimizing the trace of the Gram matrix.
				\end{enumerate}
				This option is activated by doing
				\begin{lstlisting}
P.TraceHeuristic(1) % activate the trace heuristic
				\end{lstlisting}
				Note again, that this option implies the performance estimation problem to be solved twice.
				\subsection{Working with operators}\label{sec:operators}
				In PESTO, operators are treated exactly in the same way as functions; the only difference being that trying to evaluate function values with operators will result in \verb|NaN|. The routines handling quadratic constraints and interpolation conditions for operators are provided within the \verb?Operator_classes? directory. The list of operator classes that are handled in the toolbox is presented in Table~\ref{Tab:operator_classes}.  For details about the corresponding input parameters, type \verb?help ClassName? in the Matlab prompt (e.g., \verb?help StronglyMonotone?).
				\begin{table}[ht!]
					{
						\begin{center}
							{\renewcommand{\arraystretch}{1.2}
								\begin{tabular}{@{}llc@{}}
									\specialrule{2pt}{1pt}{1pt}
									Operator class & PESTO routine name & Tightness\\
									\hline
									Monotone (maximally) &  \verb?Monotone? &  \ding{52}\\
									Strongly monotone (maximally) &  \verb?StronglyMonotone? &  \ding{52}\\
									Cocoercive &  \verb?Cocoercive? &  \ding{52}\\
									Lipschitz &  \verb?Lipschitz? &  \ding{52}\\
									Cocoercive and strongly monotone &  \verb?CocoerciveStronglyMonotone? &  \ding{54}\\
									Lipschitz and strongly monotone &  \verb?LipschitzStronglyMonotone? &  \ding{54}\\
									\specialrule{2pt}{1pt}{1pt}
								\end{tabular}
								\caption{Default operator classes within PESTO. Some classes are overlapping and are present only for promoting a better readability of the code. The corresponding interpolation conditions are developed in~\cite[Section 2]{ryu2018operator}. Also note that no interpolations condition hold for the classes of \emph{cocoercive and strongly monotone} and \emph{Lipschitz and strongly monotone} operators. Using them might provide in non-tight numerical results.}
								\label{Tab:operator_classes}}
						\end{center}}
					\end{table}
					\clearpage
					%======================================
					%									%||
					\section{Advanced operations}		%||
					%==============================		%||
					%======================================

					Although the structure of the toolbox and the basic operations (see Section~\ref{sec:basicuse}) already allow for a certain flexibility for studying a variety of first-order schemes, some \emph{advanced} operations may be used in order to model a larger panel of methods and functional classes.

					\subsection{Adding add-hoc constraints}\label{sec:constraints}
					In Section~\ref{sec:basicobjects}, we introduced the \verb?InitialCondition? procedure for introducing scalar constraints. Another possibility is to use the \verb?AddConstraint? method. There are essentially no differences between the two methods; the only reason for having both is readability. The following two codes are equivalent.\\[-1cm]
					\begin{lstlisting}
P.InitialCondition((x0-xs)^2<=1);% Add an initial condition ||x0-xs||^2<= 1
					\end{lstlisting}\vspace{-.5cm}
					\begin{lstlisting}
P.AddConstraint((x0-xs)^2<=1);% Add an initial condition ||x0-xs||^2<= 1
					\end{lstlisting}
					It is also possible to add linear matrix inequalities (LMIs) to impose a matrix $M$ to be positive semi-definite $M \succeq 0$, using the routine \verb?AddLMIConstraint?. This routine takes a cell array as input, allowing to define the matrix $M$ element by element. \\[-1cm] %  containing the expressions defining the matrix that should be positive semi-definite.
					\begin{lstlisting}
M = cell(10); 	
... 					% Defining the elements of M
P.AddLMIConstraint(M);	% Add an LMI constraint for M 
					\end{lstlisting}

					\subsection{Adding points to be interpolated}
					For modelling purposes, it can be useful to explicitly create new vectors, and link them via a function, and a coordinate/subgradient relation. More precisely, consider two vectors \verb?x? and \verb?g?, one scalar \verb?f? and the following function $F$:\\[-1cm]
					\begin{lstlisting}
% We declare one convex function
F=P.DeclareFunction('Convex');
					\end{lstlisting}
					In order to force \verb?g? and \verb?f? to be respectively a subgradient and the function value of $F$ at \verb?x?, we use the \verb?AddComponent? routine:\\[-1cm]
					\begin{lstlisting}
F.AddComponent(x,g,f); % g=grad of F at x, f=F(x)
					\end{lstlisting}
					Note that in some context (e.g., when implementing new algorithmic steps or first-order oracles), it can be useful to create new vectors or scalars with no constraints on them. This can be done using the \verb?Point? class, as follows.\\[-1cm]
					\begin{lstlisting}
x=Point('Point'); % x is a vector
f=Point('Scalar');% f is a scalar; atlernative form: f=Point('Function value')
					\end{lstlisting}
					This is for example used for implementing the projection, proximal and the linear optimization steps of the toolbox. As an example, let us consider performing a proximal (or implicit) step on $F$ from some point $x_0$, with step size $\gamma$:
					\[x=x_0-\gamma \partial F(x).\]
					This is implemented in the \verb?proximal_step? routine of PESTO. Let us have a look inside it.\\[-1cm]
					\begin{lstlisting}
function [x] = proximal_step(x0,func,gamma)
% [x] = proximal_step(x0,func,gamma)
%
% This routine performs a proximal step of step size gamma, starting from
% x0, and on function func. That is, it performs:
%       x=x0-gamma*g, where g is a (sub)gradient of func at x.
%       (implicit/proximal scheme).
%
% Input: - starting point x0
%        - function func on which the (sub)gradient will be evaluated
%        - step size gamma of the proximal step
%
% Output: x=x0-gamma*g, where g is a (sub)gradient of func at x.

g=Point('Point');
x=x0-gamma*g;
f=Point('Function value');
func.AddComponent(x,g,f);
end
					\end{lstlisting}
					\subsection{Adding new primitive oracles and primitive algorithmic steps}

					Adding new primitive oracles and algorithmic steps within PESTO is fundamentaly very simple: just add a new routines with appropriate input/output arguments. The lists of existing such routines can be found in Section~\ref{sec:oracles} and~\ref{sec:alg_steps}, and in the directories \verb?Primitive_steps? and \verb?Primitive_oracles? of the toolbox.
					\subsection{Adding new functional classes}
					First of all, we saw in Section~\ref{sec:functions} how to create new functions within some predefined classes. As an example, the following lines create a smooth strongly convex function.\\[-1cm]
					\begin{lstlisting}
paramF.mu=.1;	% Strong convexity parameter
paramF.L=1;    % Smoothness parameter
F=P.DeclareFunction('SmoothStronglyConvex',paramF);
					\end{lstlisting}
					Essentially, when creating a function, we instantiate a function object containing a list (initially empty) of points on which its corresponding interpolation conditions should hold. For encoding the interpolation condition, we use a very simple approach: each function object also refers to an \emph{interpolation routine} (all interpolation routines are presented in the directory \verb?Functions_classes? of the PESTO toolbox).
					In order to create new functions, it is also possible to directly create a instantiate a function object and associate it to a specific interpolation routine.

					As an example, the interpolation routine for the class of smooth strongly convex function is \verb?SmoothStronglyConvex.m?, and the previous code for generating a smooth strongly convex function can equivalently be written as\\[-1cm]
					\begin{lstlisting}
paramF.mu=.1;	% Strong convexity parameter
paramF.L=1;    % Smoothness parameter
F=P.AddObjective(@(pt1,pt2)SmoothStronglyConvex(pt1,pt2,paramF.mu,paramF.L));
					\end{lstlisting}
					In other words, each \emph{interpolation routine} is a method taking two points (\verb?pt1? and \verb?pt2?) in input, as well as as many parameters as needed (here, $\mu$ and $L$), and providing the interpolation constraint corresponding to those two points in output (we assume that interpolation conditions are always required for all pairs of points). In order to create a new function, one has create a \emph{function handle} depending only on the two points (\verb?pt1? and \verb?pt2?), by fixing the values of the parameters.

					Concerning the implementation of interpolation routines, note that both points \verb?pt1? and \verb?pt2? are structures with three fields corresponding to the coordinate vector: \verb?pt1.x?, its corresponding (sub)gradient: \verb?pt1.g? and its function value \verb?pt1.f?. All those elements should be treated as standard vectors or scalars of the PESTO toolbox. For example, \verb?SmoothStronglyConvex.m? contains the following code.\\[-1cm]

					\begin{lstlisting}
function cons=SmoothStronglyConvex(pt1,pt2,mu,L)
assert(mu>=0 & L>=0 & L>=mu,'Constants provided to the functional class are not valid');
if ~(pt1.x.isEqual(pt2.x) && pt1.g.isEqual(pt2.g) && pt1.f.isEqual(pt2.f))
	if L~=Inf
			cons=((pt1.f-pt2.f+pt1.g*(pt2.x-pt1.x)+...
				1/(2*(1-mu/L))*(1/L*(pt1.g-pt2.g)*(pt1.g-pt2.g).'+...
				mu*(pt1.x-pt2.x)*(pt1.x-pt2.x).'-...
				2*mu/L*(pt1.x-pt2.x)*(pt1.g-pt2.g).'))<=0);
	else
			cons=((pt1.f-pt2.f+pt1.g*(pt2.x-pt1.x)+mu/2*(pt1.x-pt2.x)^2)<=0);
	end
else
	cons=[];
end
end
					\end{lstlisting}
					Another example: \verb?Convex.m? contains the following code.\\[-1cm]
					\begin{lstlisting}
function cons=Convex(pt1,pt2)
if ~(pt1.x.isEqual(pt2.x) && pt1.g.isEqual(pt2.g) && pt1.f.isEqual(pt2.f))
	cons=((pt1.f-pt2.f+pt1.g*(pt2.x-pt1.x))<=0);
else
	cons=[];
end
end
					\end{lstlisting}
					\newpage
					\subsection{Tags and evaluations}\label{sec:tags}
					When evaluating a function value, a (sub)gradient, or both, it is possible to \emph{tag} the corresponding values, in order to be able to easily recover them. As examples,


					In some cases, tags allows recovering hidden pieces of information. For example, when minimizing $F(x)=f_1(x)+f_2(x)$ with both $f_1$ and $f_2$ being non-smooth convex functions and $x_\star$ being an optimal point of $F$.  How do we efficiently recover two vectors $g_1\in\partial f_1(x_\star)$ and $g_2\in\partial f_2(x_\star)$ such that $g_1+g_2=0$ ?\\[-1cm]
					\begin{lstlisting}
f1=P.DeclareFunction('Convex');
f2=P.DeclareFunction('Convex');
F=f1+f2; % F is the objective function

[xs,fs]=F.OptimalPoint('opt'); % xs is an optimal point, and fs=F(xs)

% note that we tag the point xs as 'opt' to be able to re-evaluate it
% easily (providing the oracle routine with this tag allows to recover
% previously evaluated points).

% the next step evaluates the oracle at the tagged point 'opt' (xs) for
% recovering the values of g1s and g2s; this allows to guarantee that
% g1s+g2s=0;
[g1s,~]=f1.oracle('opt');
[g2s,~]=f2.oracle('opt');
					\end{lstlisting}

					Note that the \verb?double? command, allowing to evaluate a vector or a scalar after solving the PEP does not allow evaluating gradients and function values that were not saved in a variable, or appropriately tagged. For example, the following lines are valid and equivalent\\[-1cm]
					\begin{lstlisting}
double(g1s), double(g2s),
double(f1.gradient('opt')), double(f2.gradient('opt'))
					\end{lstlisting}
					(note that evaluating the corresponding function values also work.\\[-1cm]
					\begin{lstlisting}
double(f1.value('opt')), double(f2.value('opt'))
					\end{lstlisting}
					Finally, note that \emph{tags} can also be specified when evaluating function and gradient values with the \verb?oracle?, \verb?subgradient? or \verb?value? routines; the three following lines have the same results.\\[-1cm]
					\begin{lstlisting}
F.oracle(x0,'x0');
F.subgradient(x0,'x0');
F.value(x0,'x0');
					\end{lstlisting}
					That is, each of those lines evaluate the function and/or gradient at $x_0$ and tag the evaluation. The evaluated gradient and function values can be recovered using one of the  following way:\\[-1cm]
					\begin{lstlisting}
[g0,F0]=F.oracle('x0');
g0=F.subgradient('x0');
F0=F.value('x0');
					\end{lstlisting}
					\begin{bclogo}[logo=\bcattention, couleur=blue!30, arrondi =0.1, sousTitre=Specifying both a tag a a point]{Good to know}
						When calling an evaluation routine (e.g., \emph{oracle}, \emph{gradient}, etc.) for evaluating (sub)gradients and/or function values, with both a point and a tag all routine will prioritize tags! Meaning that if another point with the same tag was already evaluated, the output will correspond to the evaluation of the other point. As an example:\\[-1cm]
						\begin{lstlisting}
[g0,F0]=F.oracle(x0,'x0');
[g1,F1]=F.oracle(x1,'x0');
						\end{lstlisting}
						will output $g_1=g_0$ and $f_1=f_0$ even if $x_1\neq x_0$. In other words, it will first check in the database if the point corresponding to the tag was already evaluated before performing an actual new evaluation.
					\end{bclogo}
					\clearpage
					%======================================
					%									%||
					\section{Applications}		%||
					%==============================		%||
					%======================================
					More examples and demonstration files are available within the toolbox (in the directories \verb?Examples? and \verb?Examples_CDC?). We provide the overview below; note that we included analyses done through PEPs from different authors (details in the example files)~\cite{Article:Drori,drori2014optimal,drori2014contributions,drori2017exact,drori2018efficient,kim2014optimized,kim2018optimizing,kim2015convergence,kim2019accelerated,deKlerkELS2016,taylor2015exact,taylor2015smooth,taylor2018exact,pmlr-v99-taylor19a,ryu2018operator,gu2019optimal, colla2021decentralized}, and IQCs~\cite{van2018fastest,cyrus2018robust}

					\subsection{Complete list of examples}
					The toolbox include the following examples:
					\begin{enumerate}
						\item methods for unconstrained convex minimization:
						\begin{itemize}
							\item proximal point algorithm (see e.g.,~\cite{guler1991convergence}), fast/accelerated proximal point algorithms (see~\cite{guler1992new}), subgradient methods (see e.g.,~\cite{Shor:Subgradient,Book:Nesterov2}), subgradient with exact line search (see~\cite{drori2018efficient}), gradient method (see e.g.,~\cite{Book:Nesterov2,Book:polyak1987}), gradient with exact line-search (see e.g.,~\cite{deKlerkELS2016,de2017worst}), heavy-ball method (see e.g.,~\cite{polyak1964some}), fast gradient method (see e.g.,~\cite{Nesterov:1983wy}), conjugate gradient method (see e.g.,~\cite{drori2018efficient}), optimized gradient method (see e.g.,~\cite{kim2014optimized,Article:Drori,drori2014contributions,drori2017exact}), optimized gradient method with line search (see e.g.,~\cite{drori2018efficient}), optimized gradient method for gradient norm (see e.g.,~\cite{kim2018optimizing}), fast/accelerated gradient method for smooth strongly convex minimization (see e.g.,~\cite{Book:Nesterov2,bansal2017potential}), triple momentum method (see e.g.,~\cite{van2018fastest}), robust momentum method (see e.g.,~\cite{cyrus2018robust}), relatively inexact fast gradient, gradient with exact line-search in inexact directions (see e.g.,~~\cite{deKlerkELS2016,de2017worst}).
						\end{itemize}
						\item Methods for composite convex minimization:
						\begin{itemize}
							\item proximal point method (see e.g.,~\cite{guler1991convergence}), projected and proximal gradient method (see e.g.,~\cite{Book:Nesterov2,Book:polyak1987}), fast/accelerated projected/proximal gradient method (see e.g.,~\cite{nesterov2013gradient,beck2009fast}, proximal optimized gradient method (see e.g.,~\cite{taylor2015exact}), conditional gradient/Frank-Wolfe (see e.g.,~\cite{frank1956algorithm,jaggi2013revisiting}), Douglas-Rachford (see e.g.,~\cite{douglas1956,bauschke2017douglas,giselsson2016linear}), fast/accelerated Douglas-Rachford (see e.g.,~\cite{patrinos2014douglas}), three operator splitting (see e.g.,~\cite{davis2017three}), Bregman gradient descent/NoLips (see e.g.,~\cite{bauschke2016descent,lu2018relatively,Dragomir2019optimal}), Bregman proximal point method (see e.g.,~\cite{eckstein1993nonlinear}), improved interior gradient algorithm (IGA,~\cite{auslender2006interior}).
						\end{itemize}
						\item Methods for nonconvex minimization
						\begin{itemize}
							\item gradient descent, NoLips (see e.g.,~\cite{bolte2018first}).
						\end{itemize}
						\item Methods for stochastic convex minimization:
						\begin{itemize}
							\item SAGA~\cite{defazio2014saga}, Point SAGA~\cite{defazio2016simple}, SAGA with Sampled Negative Momentum (SSNM)~\cite{pmlr-v89-zhou19c}, stochastic gradient decent (see e.g.,~ \cite{moulines2011non}).
						\end{itemize}
						\item Methods for monotone inclusions (see e.g.,~\cite{bauschke2011convex,ryu2016primer})
						\begin{itemize}
							\item proximal point method, fast/accelerated proximal point method~\cite{kim2019accelerated}, Douglas-Rachford~\cite{douglas1956,moursi2019douglas} (see e.g.~\cite{bauschke2017douglas}), three operator splitting~\cite{davis2017three}.
						\end{itemize}
						\item Methods for fixed-point problems (based on~\cite{lieder2017convergence,lieder2018})
						\begin{itemize}
							\item Krasnoselskii-Mann~\cite{mann1953mean}, Halpern iterations~\cite{halpern1967fixed}.
						\end{itemize}
						\item Verifying potential functions: (based on~\cite{pmlr-v99-taylor19a})
						\begin{itemize}
							\item potential on gradient method fast/accelerated gradient method based on that in~\cite{bansal2017potential} and~\cite{pmlr-v99-taylor19a}.
						\end{itemize}
						\item Geometry of (possibly smooth and strongly) convex functions
						\begin{itemize}
							\item Let $f$ and $g$ be two smooth strongly convex functions. What is the maximum distance between the optimum of $f+g$ and the average between the optimum of $f$ and that of $g$?
						\end{itemize}
						\item Adaptive methods:
						\begin{itemize}
							\item Polyak steps and variants using PEPs in~\cite{Barre2020Polyak}.
						\end{itemize}
						\item Inexact proximal methods
						\begin{itemize}
							\item Inexact versions of proximal point, forward-backward splitting, and Douglas-Rachford studied via PEPs in~\cite{Barre2020inexact}.
						\end{itemize}
						\item Decentralized optimization methods
						\begin{itemize}
							\item Decentralized Gradient Descent (DGD, see e.g. \cite{Nedic2009Distrib}), studied via PEPs in~\cite{colla2021decentralized}. %One exact formulation when the communication matrix is given and one relaxed formulation that considers the worst-case among entire classes of possible communication matrices, based on their spectrum. These two formulation are developed and exploited in \cite{colla2021decentralized}.
						\end{itemize}
					\end{enumerate}

					The following pages contain the code for two of them.
					\clearpage
					\subsection{Example 1: Douglas-Rachford splitting for monotone inclusion}
		\begin{lstlisting}
% In this example, we use a Douglas-Rachford splitting (DRS)
% method for solving a monotone inclusion problem
%   find x st   0 \in Ax + Bx
% where A is L-Lipschitz and monotone and B is (maximally) mu-strongly
% monotone. We denote by JA and JB the respective resolvents of A and B.
%
% One iteration of the algorithm starting from point w is as follows:
%       x = JB( w )
%       y = JA( 2 * x - w )
%       z = w - theta * ( x - y )
% and then we choose as the next iterate the value of z.
%
% Given two initial points w0 and w1, we show how to compute the worst-case
% contraction factor ||z0 - z1||/||w0 - w1|| obtained after doing one
% iteration of DRS from respectively w0 and w1.
% Note that we allow the user to choose a stepsize alpha in the resolvent.
%
% This setting is studied in
% [1] Walaa M. Moursi, and Lieven Vandenberghe. "Douglas-Rachford
%     Splitting for the Sum of a Lipschitz Continuous and a Strongly
%     Monotone Operator." (2019)
% and the methodology using PEPs is presented in
% [2] Ernest K. Ryu, Adrien B. Taylor, C. Bergeling, and P. Giselsson.
%      "Operator Splitting Performance Estimation: Tight contraction
%       factors and optimal parameter selection." (2018).
% since the results of [2] tightened that of [1] we compare with [2] below.

% (0) Initialize an empty PEP
P=pep();

% (1) Set up the class of monotone inclusions
paramA.L  =  1; paramA.mu = 0; % A is 1-Lipschitz and 0-strongly monotone
paramB.mu = .1;                % B is .1-strongly monotone

A = P.DeclareFunction('LipschitzStronglyMonotone',paramA);
B = P.DeclareFunction('StronglyMonotone',paramB);

% (2) Set up the starting points
w0=P.StartingPoint();
w1=P.StartingPoint();
P.InitialCondition((w0-w1)^2<=1);  % Normalize the initial distance ||w0-ws||^2 <= 1

% (3) Algorithm
alpha = 1.3;		% step size (in the resolvents)
theta = .9;         % overrelaxation

x0 = proximal_step(w0,B,alpha);
y0 = proximal_step(2*x0-w0,A,alpha);
z0 = w0-theta*(x0-y0);

x1 = proximal_step(w1,B,alpha);
y1 = proximal_step(2*x1-w1,A,alpha);
z1 = w1-theta*(x1-y1);

% (4) Set up the performance measure: ||z0-z1||^2
P.PerformanceMetric((z0-z1)^2);

% (5) Solve the PEP
P.solve()

% (6) Evaluate the output
double((z0-z1)^2)   % worst-case contraction factor
\end{lstlisting}
					\clearpage
\subsection{Example 2: properties of smooth strongly convex functions}
The file \verb?Examples/08_Properties of (non)convex functions/A_Minimizer of a sum.m? illustrates how the toolbox can be used to explore properties of (strongly) convex functions that are not directly related to optimization algorithms. In this example, we consider the following problem.

Given two functions $f$ and $g$ both $\mu$-strongly convex and $L$-smooth, how far can the minimizer $x_\star(f+g)$ of the sum $f+g$ be from $\tfrac12 (x_\star(f)+x_\star(g))$ (where $x_\star(f)$ and $x_\star(g)$ are respectively the minimizers of $f$ and $g$). Using the code, one can easily verify that
\begin{itemize}
	\item[(i)] the solution is invariant if $\mu$ and $L$ are multiplied by the same constant, and hence that the solution only depends on the condition number $\kappa = L/\mu$;
	\item[(ii)] the solution scales linearly with $\norm{x_\star(g)-x_\star(f)}$ which we thus assume to be $1$.
\end{itemize}
The code computes the worst-case distance $\norm{x_\star(f+g)-\tfrac{x_\star(f)+x_\star(g)}{2}}$ for various values of $\kappa$ and plots its dependence on $\kappa$.
\begin{lstlisting}
% This example shows how the PESTO toolbox can also be used to explore
% properties of (strongly) convex functions that are not directly related
% to optimization algorithms:
% we consider the following problem: Given two functions f, g both
% mu-strongly convexe and L-smooth, how far can the minimizer xsfg of (f+g)
% be from the mean of the minimizers xsf and xsg of respectively f and g.
%
% One can verify that the answer to this problem
% (i) is invariant if mu and L are multiplied by a same constant, and hence
% only depends on the condition number kappa = L/mu
% (ii) scales linearly with ||xsf-xsg||, which we will thus assume to be 1
%
% The code computes the worst-case distance for various values of kappa and
% plots its dependence on kappa.


% number of tests and vector of kappa to be tested
n_test = 80;
kappa = 1.025.^(1:n_test);
verbose = 0; % verbose mode of the toolbox


for k = 1:n_test;

	% (0) Information about current test
	disp('---------------------------------------------------')
	disp(['Test ', num2str(k) , ' of ', num2str(n_test)])
	disp(['condition number = ' , num2str(round(kappa(k)*10^3)/10^3)])
	disp('---------------------------------------------------')

	% (1) Declaration of the "PEP"
	P=pep();

	% (2) functions parameters (using scale_invariance)
	param.mu=1;	% Strong convexity parameter
	param.L=kappa(k);      % Smoothness parameter

	% (3) functions declarations
	f= P.DeclareFunction('SmoothStronglyConvex',param);
	g= P.DeclareFunction('SmoothStronglyConvex',param);
	fg = f+g;

	% (4) decleration of the minimizers
	[xsf,fs] = f.OptimalPoint();
	[xsg,gs] = g.OptimalPoint();
	[xsfg,fgs] = fg.OptimalPoint();

	% (5) Contraint on the minimizer
	P.AddConstraint((xsf-xsg)^2<=1);
	% note: the problem will naturally force (xsf-xsg)^2 = 1

	% (6) Criterion to be maximized
	P.PerformanceMetric((xsfg-(xsf+xsg)/2 )^2 );

	% (7) Solving the Pep
	P.solve(verbose);

	% (8) Evaluation and storage the output
	distance_sq(k) =double((xsfg-(xsf+xsg)/2 )^2);

end

% representation of the results
distance = sqrt(distance_sq)

figure
plot(kappa,distance_sq)
xlabel('condition number');
ylabel('||x^s_{fg} - \frac{1}{2}(x^s_f+x^s_g)||^2')
title('square distance to average minimizers f, g')

figure
plot(kappa,distance)
xlabel('condition number');
ylabel('||x^s_{fg} - \frac{1}{2}(x^s_f+x^s_g)||')
title('distance to average minimizers f, g')


disp('************************************************************')
disp('Computation ended')
disp('The result should show the square distance is asymptotically')
disp('linear in the condition number')
disp('************************************************************')

\end{lstlisting}
\clearpage
\bibliographystyle{unsrt}
\bibliography{bib_}{}
\end{document}
